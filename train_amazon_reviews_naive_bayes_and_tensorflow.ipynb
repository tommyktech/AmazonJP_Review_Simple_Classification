{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AmazonレビューのClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T07:41:58.583186Z",
     "start_time": "2021-09-18T07:41:56.612863Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# まずはデータを準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T07:41:58.755173Z",
     "start_time": "2021-09-18T07:41:58.583186Z"
    }
   },
   "outputs": [],
   "source": [
    "# データをロード\n",
    "files = {\"train\": \"amazon_reviews_train.csv\", \"test\": \"amazon_reviews_test.csv\"}\n",
    "train_df = pd.read_csv(files[\"train\"], index_col=0)\n",
    "test_df  = pd.read_csv(files[\"test\"], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T07:42:37.906304Z",
     "start_time": "2021-09-18T07:41:58.755173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17647/17647 [00:35<00:00, 502.05it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1961/1961 [00:03<00:00, 499.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# MeCab で分かち書き\n",
    "import MeCab\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "def yield_wakati_terms(text):\n",
    "    mecab = MeCab.Tagger (\"-Owakati\")\n",
    "    wakati_text = mecab.parse (text)\n",
    "    return wakati_text.strip()\n",
    "        \n",
    "train_df[\"terms\"] = train_df[\"sentence1\"].progress_map(yield_wakati_terms)\n",
    "test_df[\"terms\"]  = test_df[\"sentence1\"].progress_map(yield_wakati_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF＋Naive Bayesで分類する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T07:42:42.157094Z",
     "start_time": "2021-09-18T07:42:37.906304Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 2), token_pattern='(?u)\\\\b\\\\w+\\\\b')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDFを構築する\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u'(?u)\\\\b\\\\w+\\\\b', ngram_range=(1, 2))\n",
    "vectorizer.fit(train_df[\"terms\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T07:42:45.660009Z",
     "start_time": "2021-09-18T07:42:42.157094Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なデータを抽出\n",
    "X_train = vectorizer.transform(train_df[\"terms\"].values)\n",
    "X_test  = vectorizer.transform(test_df[\"terms\"].values)\n",
    "y_train = train_df[\"label\"].astype(float)\n",
    "y_test  = test_df[\"label\"].astype(float)\n",
    "num_labels = len(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:39:43.976070Z",
     "start_time": "2021-09-18T08:39:40.927422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'alpha': 0.01}\n",
      "best score: -0.6757729264785761\n",
      "validation log_loss: 0.5519795891567499\n",
      "validation accuracy: 0.8077511473737888\n"
     ]
    }
   ],
   "source": [
    "# モデル構築とtrainingを行う\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "parameters = [\n",
    "    {'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]}\n",
    "    ]\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    MultinomialNB(),\n",
    "    parameters,\n",
    "    cv=5,\n",
    "    scoring={\"acc\": \"accuracy\", \"logloss\": \"neg_log_loss\"}, \n",
    "    refit=\"logloss\",\n",
    "    verbose=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"best params:\", clf.best_params_)\n",
    "print(\"best score:\", clf.best_score_)\n",
    "\n",
    "y_pred_proba = clf.best_estimator_.predict_proba(X_test)\n",
    "y_pred = clf.best_estimator_.predict(X_test)\n",
    "print(\"validation log_loss:\", log_loss(y_test, y_pred_proba))\n",
    "print(\"validation accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlowで分類する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T18:12:24.221444Z",
     "start_time": "2021-09-17T18:12:24.190591Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なデータを再度抽出\n",
    "X_train = train_df[\"terms\"].values\n",
    "X_test  = test_df[\"terms\"].values\n",
    "\n",
    "# 1から5までの数値になっているのでそれを0から4に変換する\n",
    "y_train = train_df[\"label\"].astype(float) - 1\n",
    "y_test  = test_df[\"label\"].astype(float) - 1\n",
    "\n",
    "num_labels = len(y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T18:12:28.270578Z",
     "start_time": "2021-09-17T18:12:26.520011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Token化するためのレイヤーを定義\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "vocab_size = 20000\n",
    "sequence_length = 256\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dense (+Dropout) でモデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T18:15:47.141774Z",
     "start_time": "2021-09-17T18:13:28.074019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "552/552 [==============================] - 15s 26ms/step - loss: 1.3464 - acc: 0.4624 - val_loss: 1.0028 - val_acc: 0.5747\n",
      "Epoch 2/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.9074 - acc: 0.6293 - val_loss: 0.8649 - val_acc: 0.6609\n",
      "Epoch 3/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.6855 - acc: 0.7373 - val_loss: 0.8115 - val_acc: 0.6996\n",
      "Epoch 4/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.5352 - acc: 0.8016 - val_loss: 0.8100 - val_acc: 0.7190\n",
      "Epoch 5/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.4117 - acc: 0.8539 - val_loss: 0.7799 - val_acc: 0.7542\n",
      "Epoch 6/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.3389 - acc: 0.8777 - val_loss: 0.8547 - val_acc: 0.7430\n",
      "Epoch 7/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.2651 - acc: 0.9078 - val_loss: 0.9106 - val_acc: 0.7328\n",
      "Epoch 8/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.2197 - acc: 0.9227 - val_loss: 0.9047 - val_acc: 0.7817\n",
      "Epoch 9/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.1873 - acc: 0.9367 - val_loss: 1.0868 - val_acc: 0.7440\n",
      "Epoch 10/20\n",
      "552/552 [==============================] - 14s 25ms/step - loss: 0.1698 - acc: 0.9405 - val_loss: 1.0693 - val_acc: 0.7889\n",
      "min val_loss: 0.7799156308174133 val_acc: 0.7542070150375366\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=256\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # output shape=(17647, 256, 256): vocab_size, sequence_length, embedding_dim\n",
    "  GlobalAveragePooling1D(),\n",
    "  Dropout(0.2),\n",
    "  Dense(256, activation='relu'),\n",
    "  Dropout(0.2),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(num_labels, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])\n",
    "\n",
    "callbacks = [EarlyStopping(\n",
    "    monitor='val_loss', patience=5)]\n",
    "\n",
    "dense_history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "min_index = np.argmin(dense_history.history[\"val_loss\"])\n",
    "print(\"min val_loss:\", dense_history.history[\"val_loss\"][min_index], \"val_acc:\", dense_history.history[\"val_acc\"][min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T18:48:34.805136Z",
     "start_time": "2021-09-17T18:39:09.751847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "552/552 [==============================] - 74s 116ms/step - loss: 1.2721 - acc: 0.4847 - val_loss: 1.0107 - val_acc: 0.5752\n",
      "Epoch 2/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.8977 - acc: 0.6286 - val_loss: 0.8911 - val_acc: 0.6384\n",
      "Epoch 3/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.6761 - acc: 0.7402 - val_loss: 0.8487 - val_acc: 0.6747\n",
      "Epoch 4/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.4776 - acc: 0.8243 - val_loss: 0.8076 - val_acc: 0.7257\n",
      "Epoch 5/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.3466 - acc: 0.8776 - val_loss: 0.9079 - val_acc: 0.7491\n",
      "Epoch 6/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.2255 - acc: 0.9254 - val_loss: 0.9884 - val_acc: 0.7430\n",
      "Epoch 7/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.1752 - acc: 0.9422 - val_loss: 0.9742 - val_acc: 0.7828\n",
      "Epoch 8/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.1342 - acc: 0.9560 - val_loss: 1.0156 - val_acc: 0.7700\n",
      "Epoch 9/20\n",
      "552/552 [==============================] - 61s 111ms/step - loss: 0.0926 - acc: 0.9699 - val_loss: 1.2002 - val_acc: 0.7874\n",
      "min val_loss: 0.8076110482215881 val_acc: 0.7256501913070679\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, Bidirectional, LSTM, BatchNormalization\n",
    "model_lstm = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, mask_zero=True), # output shape=(17647, 256, 256): vocab_size, sequence_length, embedding_dim\n",
    "    Bidirectional(LSTM(256,  return_sequences=True)),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_labels, activation=\"softmax\")\n",
    "])\n",
    "model_lstm.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])\n",
    "lstm_history = model_lstm.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "min_index = np.argmin(lstm_history.history[\"val_loss\"])\n",
    "print(\"min val_loss:\", lstm_history.history[\"val_loss\"][min_index], \"val_acc:\", lstm_history.history[\"val_acc\"][min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TD-IDF + Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T08:38:55.783778Z",
     "start_time": "2021-09-16T08:38:51.859980Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDFでToken化するためのレイヤーを定義\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "vocab_size = 20000\n",
    "\n",
    "tfidf_vectorize_layer = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='tf-idf',\n",
    "    ngrams=(1, 2)\n",
    ")\n",
    "\n",
    "tfidf_vectorize_layer.adapt(train_df[\"terms\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-16T08:44:11.821041Z",
     "start_time": "2021-09-16T08:43:11.627117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "552/552 [==============================] - 9s 16ms/step - loss: 1.1918 - acc: 0.5405 - val_loss: 0.7081 - val_acc: 0.7307\n",
      "Epoch 2/10\n",
      "552/552 [==============================] - 9s 15ms/step - loss: 0.4154 - acc: 0.8549 - val_loss: 0.6192 - val_acc: 0.7904\n",
      "Epoch 3/10\n",
      "552/552 [==============================] - 8s 15ms/step - loss: 0.1679 - acc: 0.9505 - val_loss: 0.6515 - val_acc: 0.8072\n",
      "Epoch 4/10\n",
      "552/552 [==============================] - 8s 15ms/step - loss: 0.1165 - acc: 0.9744 - val_loss: 0.8522 - val_acc: 0.8123\n",
      "Epoch 5/10\n",
      "552/552 [==============================] - 9s 15ms/step - loss: 0.0686 - acc: 0.9832 - val_loss: 0.8639 - val_acc: 0.8032\n",
      "Epoch 6/10\n",
      "552/552 [==============================] - 8s 15ms/step - loss: 0.0461 - acc: 0.9876 - val_loss: 0.8379 - val_acc: 0.8134\n",
      "Epoch 7/10\n",
      "552/552 [==============================] - 9s 15ms/step - loss: 0.0659 - acc: 0.9834 - val_loss: 0.9092 - val_acc: 0.8083\n",
      "min val_loss: 0.6191643476486206 val_acc: 0.7904130816459656\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = Sequential([\n",
    "    tfidf_vectorize_layer,\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_labels, activation=\"softmax\")\n",
    "])\n",
    "model_tfidf.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])\n",
    "tfidf_history = model_tfidf.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks)\n",
    "\n",
    "min_index = np.argmin(tfidf_history.history[\"val_loss\"])\n",
    "print(\"min val_loss:\", tfidf_history.history[\"val_loss\"][min_index], \"val_acc:\", tfidf_history.history[\"val_acc\"][min_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考. nnlm-ja-dim128 + Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T15:25:16.891889Z",
     "start_time": "2021-09-17T15:24:30.120344Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17647/17647 [00:42<00:00, 420.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1961/1961 [00:04<00:00, 414.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 必要なデータを再度抽出\n",
    "X_train = train_df[\"terms\"].values\n",
    "X_test  = test_df[\"terms\"].values\n",
    "y_train = train_df[\"label\"].astype(float) - 1\n",
    "y_test  = test_df[\"label\"].astype(float) - 1\n",
    "num_labels = len(y_train.unique())\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T15:53:01.500058Z",
     "start_time": "2021-09-17T15:53:00.202945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001708447E438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000001708447E438> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000170A8F94948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000170A8F94948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_6 (KerasLayer)   (None, 128)               117568128 \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 117,570,277\n",
      "Trainable params: 2,149\n",
      "Non-trainable params: 117,568,128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-ja-dim128-with-normalization/2\",\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "model = Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T15:53:34.374421Z",
     "start_time": "2021-09-17T15:53:03.661617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "35/35 [==============================] - 1s 19ms/step - loss: 1.5659 - acc: 0.2739 - val_loss: 1.4369 - val_acc: 0.4294\n",
      "Epoch 2/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.4162 - acc: 0.4337 - val_loss: 1.4053 - val_acc: 0.4314\n",
      "Epoch 3/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.3797 - acc: 0.4407 - val_loss: 1.3656 - val_acc: 0.4544\n",
      "Epoch 4/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.3416 - acc: 0.4657 - val_loss: 1.3214 - val_acc: 0.4686\n",
      "Epoch 5/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.2891 - acc: 0.4951 - val_loss: 1.2803 - val_acc: 0.5105\n",
      "Epoch 6/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.2677 - acc: 0.5044 - val_loss: 1.2466 - val_acc: 0.5161\n",
      "Epoch 7/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.2212 - acc: 0.5181 - val_loss: 1.2197 - val_acc: 0.5201\n",
      "Epoch 8/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.2108 - acc: 0.5224 - val_loss: 1.2025 - val_acc: 0.5186\n",
      "Epoch 9/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1940 - acc: 0.5266 - val_loss: 1.1877 - val_acc: 0.5263\n",
      "Epoch 10/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1698 - acc: 0.5374 - val_loss: 1.1773 - val_acc: 0.5329\n",
      "Epoch 11/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1610 - acc: 0.5360 - val_loss: 1.1671 - val_acc: 0.5400\n",
      "Epoch 12/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1588 - acc: 0.5337 - val_loss: 1.1625 - val_acc: 0.5273\n",
      "Epoch 13/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1530 - acc: 0.5336 - val_loss: 1.1549 - val_acc: 0.5349\n",
      "Epoch 14/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1428 - acc: 0.5368 - val_loss: 1.1490 - val_acc: 0.5421\n",
      "Epoch 15/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1370 - acc: 0.5393 - val_loss: 1.1508 - val_acc: 0.5298\n",
      "Epoch 16/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1217 - acc: 0.5437 - val_loss: 1.1413 - val_acc: 0.5395\n",
      "Epoch 17/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1327 - acc: 0.5442 - val_loss: 1.1410 - val_acc: 0.5370\n",
      "Epoch 18/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1286 - acc: 0.5452 - val_loss: 1.1380 - val_acc: 0.5385\n",
      "Epoch 19/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1293 - acc: 0.5428 - val_loss: 1.1334 - val_acc: 0.5411\n",
      "Epoch 20/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1290 - acc: 0.5423 - val_loss: 1.1324 - val_acc: 0.5436\n",
      "Epoch 21/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1258 - acc: 0.5445 - val_loss: 1.1303 - val_acc: 0.5405\n",
      "Epoch 22/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1226 - acc: 0.5471 - val_loss: 1.1292 - val_acc: 0.5405\n",
      "Epoch 23/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1133 - acc: 0.5468 - val_loss: 1.1285 - val_acc: 0.5395\n",
      "Epoch 24/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.1227 - acc: 0.5437 - val_loss: 1.1252 - val_acc: 0.5533\n",
      "Epoch 25/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1216 - acc: 0.5454 - val_loss: 1.1241 - val_acc: 0.5456\n",
      "Epoch 26/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.1234 - acc: 0.5454 - val_loss: 1.1222 - val_acc: 0.5467\n",
      "Epoch 27/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1129 - acc: 0.5518 - val_loss: 1.1224 - val_acc: 0.5426\n",
      "Epoch 28/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1142 - acc: 0.5509 - val_loss: 1.1217 - val_acc: 0.5497\n",
      "Epoch 29/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1123 - acc: 0.5461 - val_loss: 1.1218 - val_acc: 0.5467\n",
      "Epoch 30/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1046 - acc: 0.5533 - val_loss: 1.1205 - val_acc: 0.5441\n",
      "Epoch 31/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1119 - acc: 0.5484 - val_loss: 1.1166 - val_acc: 0.5507\n",
      "Epoch 32/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1104 - acc: 0.5527 - val_loss: 1.1168 - val_acc: 0.5507\n",
      "Epoch 33/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1113 - acc: 0.5478 - val_loss: 1.1151 - val_acc: 0.5487\n",
      "Epoch 34/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1113 - acc: 0.5478 - val_loss: 1.1155 - val_acc: 0.5492\n",
      "Epoch 35/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1074 - acc: 0.5493 - val_loss: 1.1146 - val_acc: 0.5477\n",
      "Epoch 36/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1151 - acc: 0.5473 - val_loss: 1.1156 - val_acc: 0.5467\n",
      "Epoch 37/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.0986 - acc: 0.5601 - val_loss: 1.1126 - val_acc: 0.5538\n",
      "Epoch 38/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.1039 - acc: 0.5501 - val_loss: 1.1121 - val_acc: 0.5497\n",
      "Epoch 39/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1013 - acc: 0.5517 - val_loss: 1.1147 - val_acc: 0.5482\n",
      "Epoch 40/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1029 - acc: 0.5563 - val_loss: 1.1107 - val_acc: 0.5523\n",
      "Epoch 41/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.0946 - acc: 0.5525 - val_loss: 1.1157 - val_acc: 0.5472\n",
      "Epoch 42/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1074 - acc: 0.5460 - val_loss: 1.1093 - val_acc: 0.5502\n",
      "Epoch 43/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.0979 - acc: 0.5514 - val_loss: 1.1079 - val_acc: 0.5523\n",
      "Epoch 44/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1057 - acc: 0.5480 - val_loss: 1.1109 - val_acc: 0.5497\n",
      "Epoch 45/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.1097 - acc: 0.5542 - val_loss: 1.1109 - val_acc: 0.5472\n",
      "Epoch 46/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.0880 - acc: 0.5627 - val_loss: 1.1086 - val_acc: 0.5543\n",
      "Epoch 47/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.0962 - acc: 0.5517 - val_loss: 1.1089 - val_acc: 0.5543\n",
      "Epoch 48/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.1031 - acc: 0.5472 - val_loss: 1.1055 - val_acc: 0.5543\n",
      "Epoch 49/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.0957 - acc: 0.5473 - val_loss: 1.1069 - val_acc: 0.5538\n",
      "Epoch 50/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.1020 - acc: 0.5542 - val_loss: 1.1078 - val_acc: 0.5528\n",
      "Epoch 51/200\n",
      "35/35 [==============================] - 1s 16ms/step - loss: 1.0884 - acc: 0.5596 - val_loss: 1.1067 - val_acc: 0.5538\n",
      "Epoch 52/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.0951 - acc: 0.5551 - val_loss: 1.1059 - val_acc: 0.5553\n",
      "Epoch 53/200\n",
      "35/35 [==============================] - 1s 17ms/step - loss: 1.0900 - acc: 0.5600 - val_loss: 1.1065 - val_acc: 0.5676\n",
      "min val_loss: 1.1055011749267578 val_acc: 0.5543090105056763\n"
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(\n",
    "    monitor='val_loss', patience=5)]\n",
    "\n",
    "history = model.fit(train_dataset.shuffle(10000).batch(512),\n",
    "    validation_data=test_dataset.batch(512),\n",
    "    epochs=200,\n",
    "    callbacks=callbacks)\n",
    "min_index = np.argmin(history.history[\"val_loss\"])\n",
    "print(\"min val_loss:\", history.history[\"val_loss\"][min_index], \"val_acc:\", history.history[\"val_acc\"][min_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
