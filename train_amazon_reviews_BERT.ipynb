{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT でアマゾンレビューを分類してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:25:23.094662Z",
     "start_time": "2021-09-18T08:25:20.530307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') memory growth: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    TFTrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "# GPU 設定\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:25:24.392254Z",
     "start_time": "2021-09-18T08:25:23.094662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3dccccb3fde64d7f\n",
      "Reusing dataset csv (tf_cache\\csv\\default-3dccccb3fde64d7f\\0.0.0\\2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "data_files = {\"train\": \"amazon_reviews_train.csv\", \"validation\": \"amazon_reviews_test.csv\"}\n",
    "datasets = load_dataset(\"csv\", data_files=data_files, cache_dir=\"tf_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:25:25.642713Z",
     "start_time": "2021-09-18T08:25:24.392254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d151ac2991489d8b94d5ff0c50fa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419294ee827f41bc8d394348c333cf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf51424504714be195426687d2dbb9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17647.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcd1487bbb0457ca0ec9f003c62f3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1961.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'label': Value(dtype='float64', id=None),\n",
       "  'sentence1': Value(dtype='string', id=None)},\n",
       " {'label': Value(dtype='float64', id=None),\n",
       "  'sentence1': Value(dtype='string', id=None)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast label from int to float\n",
    "from datasets import ClassLabel, Value\n",
    "new_features = datasets[\"train\"].features.copy()\n",
    "new_features[\"label\"] = Value('float64')\n",
    "datasets[\"train\"] = datasets[\"train\"].cast(new_features)\n",
    "\n",
    "new_features = datasets[\"validation\"].features.copy()\n",
    "new_features[\"label\"] = Value('float64')\n",
    "datasets[\"validation\"] = datasets[\"validation\"].cast(new_features)\n",
    "\n",
    "# remove unnecessary column\n",
    "datasets = datasets.map(lambda example: {'sentence1': example['sentence1']}, remove_columns=['Unnamed: 0'])\n",
    "\n",
    "datasets[\"train\"].features, datasets[\"validation\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:25:36.440456Z",
     "start_time": "2021-09-18T08:25:25.642713Z"
    }
   },
   "outputs": [],
   "source": [
    "# define tokenizer\n",
    "\n",
    "bert_folder = \"cl-tohoku/bert-base-japanese-v2\"\n",
    "# bert_folder = \"cl-tohoku/bert-base-japanese-char\"\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    bert_folder,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    ")\n",
    "\n",
    "# ラベル変換\n",
    "def build_label_convertor(labels:list):\n",
    "    labels = np.unique(labels)\n",
    "    labels = sorted(labels)\n",
    "    label2id = {v:k for k, v in enumerate(labels)}\n",
    "    return label2id\n",
    "\n",
    "label2id = build_label_convertor(datasets[\"train\"][\"label\"])\n",
    "num_labels = len(label2id)\n",
    "\n",
    "\n",
    "# config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    bert_folder,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=None,\n",
    ")\n",
    "config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:25:47.478901Z",
     "start_time": "2021-09-18T08:25:36.440456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6205bf42459546d1a0d6887d5e840a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=18.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70d1f6e244846bc9562a0c145e8540f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'sentence1', 'token_type_ids'],\n",
       "        num_rows: 17647\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'label', 'sentence1', 'token_type_ids'],\n",
       "        num_rows: 1961\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizeする\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    args = ((examples[\"sentence1\"],))\n",
    "    result = tokenizer(*args, max_length=max_seq_length, truncation=True)\n",
    "    \n",
    "    # Map labels to IDs\n",
    "    if config.label2id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(config.label2id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "max_seq_length = 128\n",
    "batch_size = 10\n",
    "processed_datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=False)\n",
    "processed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:26:00.181899Z",
     "start_time": "2021-09-18T08:25:47.478901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <MapDataset shapes: ({token_type_ids: (10, None), attention_mask: (10, None), input_ids: (10, None)}, (10,)), types: ({token_type_ids: tf.int32, attention_mask: tf.int32, input_ids: tf.int32}, tf.float64)>,\n",
       " 'validation': <MapDataset shapes: ({token_type_ids: (10, None), attention_mask: (10, None), input_ids: (10, None)}, (10,)), types: ({token_type_ids: tf.int32, attention_mask: tf.int32, input_ids: tf.int32}, tf.float64)>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データをtensorflowの形式に変換する\n",
    "def convert_dataset_for_tensorflow(\n",
    "    dataset, batch_size, dataset_mode=\"variable_batch\", drop_remainder=True\n",
    "):\n",
    "    column_names = {col for col in dataset.column_names}\n",
    "    non_label_column_names = [name for name in column_names if name not in [\"label\", 'token_type_ids', 'input_ids', 'attention_mask']]\n",
    "\n",
    "    def densify_ragged_batch(features, label=None):\n",
    "        features = {\n",
    "            feature: ragged_tensor.to_tensor(shape=batch_shape[feature]) for feature, ragged_tensor in features.items()\n",
    "        }\n",
    "        if label is None:\n",
    "            return features\n",
    "        else:\n",
    "            return features, label\n",
    "    \n",
    "    feature_keys = list(set(dataset.features.keys()) - set(non_label_column_names + [\"label\"]))\n",
    "    if dataset_mode == \"variable_batch\":\n",
    "        batch_shape = {key: None for key in feature_keys}\n",
    "        data = {key: tf.ragged.constant(dataset[key]) for key in feature_keys}\n",
    "    elif dataset_mode == \"constant_batch\":\n",
    "        data = {key: tf.ragged.constant(dataset[key]) for key in feature_keys}\n",
    "        batch_shape = {\n",
    "            key: tf.concat(([batch_size], ragged_tensor.bounding_shape()[1:]), axis=0)\n",
    "            for key, ragged_tensor in data.items()\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset mode!\")\n",
    "    \n",
    "    if \"label\" in dataset.features:\n",
    "        labels = tf.convert_to_tensor(np.array(dataset[\"label\"]))\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    else:\n",
    "        tf_dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "        \n",
    "    tf_dataset = tf_dataset.shuffle(buffer_size=len(dataset))\n",
    "    tf_dataset = tf_dataset.batch(batch_size=batch_size, drop_remainder=drop_remainder).map(densify_ragged_batch)\n",
    "    return tf_dataset\n",
    "\n",
    "\n",
    "tf_data = dict()\n",
    "for key in processed_datasets:\n",
    "    tf_data[key] = convert_dataset_for_tensorflow(processed_datasets[key], batch_size)\n",
    "tf_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T08:26:02.340396Z",
     "start_time": "2021-09-18T08:26:00.181899Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-v2 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    bert_folder,\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=False,\n",
    ")\n",
    "\n",
    "import math\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-5\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-18T09:22:46.314644Z",
     "start_time": "2021-09-18T08:26:02.340396Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001C25E92CDD8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001C25E92CDD8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1764/1764 [==============================] - ETA: 0s - loss: 1.0854 - accuracy: 0.5548WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "1764/1764 [==============================] - 690s 384ms/step - loss: 1.0854 - accuracy: 0.5548 - val_loss: 0.8023 - val_accuracy: 0.6714\n",
      "Epoch 2/5\n",
      "1764/1764 [==============================] - 677s 384ms/step - loss: 0.7174 - accuracy: 0.7145 - val_loss: 0.7507 - val_accuracy: 0.7133\n",
      "Epoch 3/5\n",
      "1764/1764 [==============================] - 678s 385ms/step - loss: 0.4343 - accuracy: 0.8383 - val_loss: 0.6475 - val_accuracy: 0.7888\n",
      "Epoch 4/5\n",
      "1764/1764 [==============================] - 678s 384ms/step - loss: 0.2454 - accuracy: 0.9167 - val_loss: 0.7399 - val_accuracy: 0.7566\n",
      "Epoch 5/5\n",
      "1764/1764 [==============================] - 679s 385ms/step - loss: 0.1566 - accuracy: 0.9470 - val_loss: 0.6181 - val_accuracy: 0.8184\n",
      "val_loss: 0.618066132068634 val_acc: 0.8183673620223999\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "history = model.fit(\n",
    "    tf_data[\"train\"],\n",
    "    validation_data=tf_data[\"validation\"],\n",
    "    epochs=5,\n",
    ")\n",
    "min_index = np.argmin(history.history[\"val_loss\"])\n",
    "print(\"val_loss:\", history.history[\"val_loss\"][min_index], \"val_acc:\", history.history[\"val_accuracy\"][min_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
